"""
Mock LLM responses for demo/offline mode
Pre-written answers that demonstrate the system without API calls
"""

import logging
from typing import List

logger = logging.getLogger(__name__)


class MockLLMService:
    """
    Mock LLM service with pre-written demo answers

    Returns realistic answers based on query keywords
    Demonstrates the full RAG pipeline without OpenAI API calls
    """

    def __init__(self):
        self.model = "mock-gpt-4o-mini"

        # Pre-written demo answers for common queries
        self.demo_answers = {
            "ros 2": """ROS 2 (Robot Operating System 2) is a next-generation robotics middleware framework that enables distributed computing and real-time communication between robot components. [Source 1]

Unlike ROS 1, ROS 2 uses DDS (Data Distribution Service) for communication, providing better reliability and security for production robotics applications. [Source 2]

Key features include:
- Real-time capabilities for safety-critical systems
- Improved security with authentication and encryption
- Better multi-robot coordination
- Cross-platform support (Linux, Windows, macOS)

[DEMO MODE: This is a mock response demonstrating the RAG system]""",

            "gazebo": """Gazebo is a powerful 3D robotics simulator used for creating digital twins of robotic systems. [Source 1]

It provides physics simulation, sensor simulation, and realistic rendering, making it ideal for testing robot behaviors in virtual environments before deploying to hardware. [Source 2]

Integration with ROS 2 allows seamless transition between simulation and real robots.

[DEMO MODE: Mock response showing multi-source citation]""",

            "nvidia isaac": """NVIDIA Isaac Sim is an advanced robotics simulation platform built on NVIDIA Omniverse. [Source 1]

It leverages GPU acceleration for:
- Photorealistic rendering
- Physics simulation with PhysX
- Synthetic data generation for AI training
- Real-time sensor simulation (cameras, LiDAR, IMUs)

Isaac Sim is particularly powerful for developing AI-powered perception and navigation systems for humanoid robots. [Source 2]

[DEMO MODE: This demonstrates the agent system architecture]""",

            "vla": """Vision-Language-Action (VLA) models integrate three modalities to enable robots to understand visual scenes, interpret natural language instructions, and generate appropriate actions. [Source 1]

VLA architectures typically consist of:
1. Vision encoder (e.g., ViT) for image understanding
2. Language model (e.g., BERT, GPT) for instruction parsing
3. Action decoder for generating robot control commands

This enables intuitive human-robot interaction through natural language. [Source 2]

[DEMO MODE: Full-book search demonstration]""",

            "selected_text": """Based on the text you selected, here's an explanation:

{selected_text_preview}

This section discusses {topic} which is relevant to {context}. The key points are:
- {point1}
- {point2}

[DEMO MODE: Selected text mode - showing context-restricted answering]

Would you like me to search the full book for more details?""",

            "default": """I found information related to your question in the textbook. [Source 1]

The textbook covers this topic in the context of physical AI and humanoid robotics, explaining both theoretical concepts and practical implementation details. [Source 2]

Key points include:
- Fundamental concepts and definitions
- Technical implementation approaches
- Best practices and common pitfalls

For more detailed information, please refer to the specific chapter cited in the sources.

[DEMO MODE: This is a mock response. In production, this would be generated by GPT-4o-mini based on retrieved context]""",
        }

    def generate_answer(self, query: str, context_texts: List[str], is_selected_text: bool = False) -> str:
        """
        Generate a mock answer based on query keywords

        Args:
            query: User question
            context_texts: Retrieved context chunks (ignored in demo mode)
            is_selected_text: Whether this is selected text mode

        Returns:
            Mock answer that demonstrates the system
        """
        query_lower = query.lower()

        logger.info(f"[DEMO MODE] Generating mock answer for: {query[:50]}...")

        # Selected text mode
        if is_selected_text and context_texts:
            selected_preview = context_texts[0][:200] + "..." if len(context_texts[0]) > 200 else context_texts[0]
            answer = self.demo_answers["selected_text"].format(
                selected_text_preview=selected_preview,
                topic="robotics concepts",
                context="the Physical AI curriculum",
                point1="Core technical concepts",
                point2="Practical implementation guidance"
            )
            return answer

        # Match query to pre-written answers
        if "ros" in query_lower or "ros2" in query_lower or "ros 2" in query_lower:
            return self.demo_answers["ros 2"]
        elif "gazebo" in query_lower or "digital twin" in query_lower:
            return self.demo_answers["gazebo"]
        elif "isaac" in query_lower or "nvidia" in query_lower:
            return self.demo_answers["nvidia isaac"]
        elif "vla" in query_lower or "vision" in query_lower or "language" in query_lower:
            return self.demo_answers["vla"]
        else:
            return self.demo_answers["default"]


def get_mock_llm_service() -> MockLLMService:
    """Get mock LLM service instance"""
    return MockLLMService()
